{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f1865-d49c-4928-a8d7-12660442f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from os.path import basename, join, exists\n",
    "import pickle,cv2, random, datetime, copy, collections, csv, sklearn, os, warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from scipy.ndimage import zoom \n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import layers\n",
    "from time import time\n",
    "from numpy.random import seed\n",
    "from skimage import exposure, transform\n",
    "from skimage.transform import rotate\n",
    "from scipy import ndimage, misc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb52399-276e-47da-8c2a-870d1013eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    #  kernel_regularizer='l1_l2'\n",
    "    x1 = Conv2D(filters = n_filters, kernel_size = (3, 3),\n",
    "                kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    \n",
    "\n",
    "    # second layer\n",
    "    x2 = Conv2D(filters = n_filters, kernel_size = (3, 3),\n",
    "                kernel_initializer = 'he_normal', padding = 'same')(x1)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    \n",
    "    x3 = SqueezeExcite(x2, n_filters//2)\n",
    "    \n",
    "    return x3\n",
    "\n",
    "def SqueezeExcite(x, ratio=16):\n",
    "    nb_chan = K.int_shape(x)[-1]\n",
    "\n",
    "    y = layers.GlobalAveragePooling2D()(x)\n",
    "    y = layers.Dense(nb_chan // ratio, activation=tf.keras.layers.LeakyReLU(alpha=0.3))(y)\n",
    "    y = layers.Dense(nb_chan, activation='sigmoid')(y)\n",
    "\n",
    "    y = layers.Multiply()([x, y])\n",
    "    return y\n",
    "\n",
    "\n",
    "def rrb_block2(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    nb_chan = K.int_shape(input_tensor)[-1]\n",
    "    x1 = Conv2D(filters = nb_chan, kernel_size=(3, 3), kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "\n",
    "    # second layer\n",
    "    x2 = Conv2D(filters = nb_chan,kernel_size=(3, 3), kernel_initializer = 'he_normal', padding = 'same')(x1)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    \n",
    "    x3 = Conv2D(filters = nb_chan, kernel_size=(1, 1), kernel_initializer = 'he_normal', padding = 'same')(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = tf.keras.layers.LeakyReLU()(x3)\n",
    "    \n",
    "    x4 = SqueezeExcite(x3, ratio=2)\n",
    "    \n",
    "    out1 = layers.Add()([x1, x2, x4])\n",
    "    \n",
    "    nb_chan = K.int_shape(x3)[-1]\n",
    "    \n",
    "    y = Conv2D(nb_chan, 1, activation = 'sigmoid', padding = 'same')(out1)\n",
    "    \n",
    "    y = layers.Multiply()([y, input_tensor])\n",
    "        \n",
    "    return y\n",
    "\n",
    "def rrb_block1(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    x1 = Conv2D(filters = n_filters, kernel_size=(3, 3), kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "\n",
    "    # second layer\n",
    "    x2 = Conv2D(filters = n_filters,kernel_size=(3, 3), kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    \n",
    "    x3 = Conv2D(filters = n_filters, kernel_size=(1, 1), kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "\n",
    "    out = layers.Add()([x1, x2, x3])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db541a5b-b15e-4152-ad27-64a7d1c8b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate\n",
    "\n",
    "def ASPP_block(inputs):\n",
    "    \"\"\"Implementation of Atrous Spatial Pyramid Pooling block\"\"\"\n",
    "    \n",
    "    # Define the atrous convolution rates\n",
    "    rates = [3, 6, 9]\n",
    "    \n",
    "    # Apply 1x1 convolution\n",
    "    conv_1x1 = Conv2D(filters=64, kernel_size=1, activation='relu')(inputs)\n",
    "    \n",
    "    # Apply 3x3 atrous convolutions with different rates\n",
    "    conv_3x3_rate_1 = Conv2D(filters=64, kernel_size=3, dilation_rate=rates[0], padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.3))(inputs)\n",
    "    conv_3x3_rate_2 = Conv2D(filters=64, kernel_size=3, dilation_rate=rates[1], padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.3))(inputs)\n",
    "    conv_3x3_rate_3 = Conv2D(filters=64, kernel_size=3, dilation_rate=rates[2], padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.3))(inputs)\n",
    "    \n",
    "    # Concatenate the output of all convolutions\n",
    "    concat = Concatenate()([conv_1x1, conv_3x3_rate_1, conv_3x3_rate_2, conv_3x3_rate_3])\n",
    "    \n",
    "    # Apply 1x1 convolution\n",
    "    aspp_out = Conv2D(filters=64, kernel_size=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.3))(concat)\n",
    "    \n",
    "    return aspp_out\n",
    "\n",
    "\n",
    "def res_block(x, n_filters, count):\n",
    "    if count == 4:\n",
    "        \n",
    "        x1 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "        x2 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "        x3 = Add()([x1, x2])\n",
    "        \n",
    "        x4 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x3)\n",
    "        x4 = BatchNormalization()(x4)\n",
    "        x4 = tf.keras.layers.LeakyReLU()(x4)\n",
    "        x5 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x3)\n",
    "        x5 = BatchNormalization()(x5)\n",
    "        x5 = tf.keras.layers.LeakyReLU()(x5)\n",
    "        x6 = Add()([x4, x5])\n",
    "        \n",
    "        x7 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x6)\n",
    "        x7 = BatchNormalization()(x7)\n",
    "        x7 = tf.keras.layers.LeakyReLU()(x7)\n",
    "        x8 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x6)\n",
    "        x8 = BatchNormalization()(x8)\n",
    "        x8 =tf.keras.layers.LeakyReLU()(x8)\n",
    "        x9 = Add()([x7, x8])\n",
    "        \n",
    "        x10 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x9)\n",
    "        x10 = BatchNormalization()(x10)\n",
    "        x10 = tf.keras.layers.LeakyReLU()(x10)\n",
    "        x11 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x9)\n",
    "        x11 = BatchNormalization()(x11)\n",
    "        x11 = tf.keras.layers.LeakyReLU()(x11)\n",
    "        x12 = Add()([x10, x11])\n",
    "        \n",
    "        x13 = SqueezeExcite(x12, n_filters//4)\n",
    "        return x13\n",
    "    \n",
    "    elif count == 3:\n",
    "        \n",
    "        x1 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "        x2 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "        x3 = Add()([x1, x2])\n",
    "        \n",
    "        x4 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x3)\n",
    "        x4 = BatchNormalization()(x4)\n",
    "        x4 = tf.keras.layers.LeakyReLU()(x4)\n",
    "        x5 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x3)\n",
    "        x5 = BatchNormalization()(x5)\n",
    "        x5 = tf.keras.layers.LeakyReLU()(x5)\n",
    "        x6 = Add()([x4, x5])\n",
    "        \n",
    "        x7 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x6)\n",
    "        x7 = BatchNormalization()(x7)\n",
    "        x7 = tf.keras.layers.LeakyReLU()(x7)\n",
    "        x8 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x6)\n",
    "        x8 = BatchNormalization()(x8)\n",
    "        x8 = tf.keras.layers.LeakyReLU()(x8)\n",
    "        x9 = Add()([x7, x8])\n",
    "        \n",
    "        x10 = SqueezeExcite(x9, n_filters//4)\n",
    "        \n",
    "        return x10\n",
    "    \n",
    "    elif count == 2:\n",
    "        \n",
    "        x1 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "        x2 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "        x3 = Add()([x1, x2])\n",
    "        \n",
    "        x4 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x3)\n",
    "        x4 = BatchNormalization()(x4)\n",
    "        x4 = tf.keras.layers.LeakyReLU()(x4)\n",
    "        x5 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x3)\n",
    "        x5 = BatchNormalization()(x5)\n",
    "        x5 = tf.keras.layers.LeakyReLU()(x5)\n",
    "        x6 = Add()([x4, x5])\n",
    "        \n",
    "        x7 = SqueezeExcite(x6, n_filters//4)\n",
    "        \n",
    "        return x7\n",
    "    elif count == 1:\n",
    "        \n",
    "        x1 = Conv2D(filters = n_filters, kernel_size = (3, 3),kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "        x2 = Conv2D(filters = n_filters, kernel_size = (1, 1),kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "        x3 = Add()([x1, x2])\n",
    "        \n",
    "        x4 = SqueezeExcite(x3, n_filters//4)\n",
    "        \n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6632ca2-b2b5-4008-a080-60f8dcbad9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import ZeroPadding2D,BatchNormalization,MaxPooling2D, UpSampling2D,Dropout,multiply, Concatenate, Conv2D, Add, Activation\n",
    "\n",
    "def UNet_Seg(input_img, n_filters = 32, dropout = 0.2, batchnorm = True):\n",
    "# set image specifics\n",
    "    k = 3 # kernel size\n",
    "    img_ch = 3 # image channels\n",
    "    out_ch = 1 # output channel\n",
    "\n",
    "    c1 = conv2d_block(input_img, n_filters * 1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "    p1 = layers.Dropout(dropout)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters * 2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "    p2 = layers.Dropout(dropout)(p2)\n",
    "    \n",
    "    c11 = layers.MaxPooling2D((4, 4))(c1)\n",
    "    c3 = conv2d_block(p2, n_filters * 4)\n",
    "    c31 = layers.Concatenate()([c3, res_block(c11, n_filters * 4, 4)])\n",
    "    c32 = rrb_block2(c31, n_filters * 4, kernel_size=3, batchnorm=batchnorm)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c32)\n",
    "    p3 = layers.Dropout(dropout)(p3)\n",
    "    \n",
    "    \n",
    "    c22 = layers.MaxPooling2D((4, 4))(c2)    \n",
    "    c4 = conv2d_block(p3, n_filters * 6)\n",
    "    c41 = layers.Concatenate()([c4, res_block(c22, n_filters * 6, 3)])\n",
    "    c42 = rrb_block2(c41, n_filters * 4, kernel_size=3, batchnorm=batchnorm)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c42)\n",
    "    p4 = layers.Dropout(dropout)(p4)\n",
    "\n",
    "\n",
    "    # D1\n",
    "    c51 = ASPP_block(p4)\n",
    "    c52 = Dropout(dropout)(c51)\n",
    "\n",
    "\n",
    "    # Decoder path\n",
    "    u6 = layers.UpSampling2D(size=(2, 2))(c52)\n",
    "    u62 = layers.Concatenate()([u6, res_block(c4, n_filters * 6, 1)])\n",
    "    u63 = conv2d_block(u62, n_filters * 6)\n",
    "    u64 = rrb_block2(u63, n_filters * 6, kernel_size=3, batchnorm=batchnorm)\n",
    "    d65 = Dropout(dropout)(u64)\n",
    "\n",
    "\n",
    "    \n",
    "    u7 = layers.UpSampling2D(size=(2, 2))(d65)\n",
    "    u72 = layers.Concatenate()([u7, res_block(c3,n_filters * 4, 2)])\n",
    "    u73 = conv2d_block(u72, n_filters * 4)\n",
    "    u74 = rrb_block2(u73, n_filters * 4, kernel_size=3, batchnorm=batchnorm)\n",
    "    d75 = Dropout(dropout)(u74)\n",
    "\n",
    "    \n",
    "    ### Decoder Full_Scale Up_Conv ####\n",
    "    f1 = layers.UpSampling2D(size=(4, 4))(d65)\n",
    "    ####################################\n",
    "    \n",
    "    u8 = layers.UpSampling2D(size=(2, 2))(d75)\n",
    "    u82 = layers.Concatenate()([u8, f1, res_block(c2, n_filters * 2, 3)])\n",
    "    u83 = conv2d_block(u82, n_filters * 2)\n",
    "    u84 = rrb_block2(u83, n_filters * 2, kernel_size=3, batchnorm=batchnorm)\n",
    "    d85 = Dropout(dropout)(u84)\n",
    "\n",
    "    \n",
    "    ### Decoder Full_Scale Up_Conv ####\n",
    "    f2 = layers.UpSampling2D(size=(4, 4))(d75)\n",
    "    #####################################\n",
    "    \n",
    "    \n",
    "    u9 = layers.UpSampling2D(size=(2, 2))(d85)\n",
    "    u92 = layers.Concatenate()([u9, f2, res_block(c1, n_filters * 1, 4)])\n",
    "    u93 = conv2d_block(u92, n_filters * 1)\n",
    "    u94 = rrb_block2(u93, n_filters * 1, kernel_size=3, batchnorm=batchnorm)\n",
    "    d95 = Dropout(dropout)(u94)\n",
    "\n",
    "   \n",
    "\n",
    "    output1 = Conv2D(out_ch, (1, 1), padding='same', activation='sigmoid', name=\"out1\")(d95)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_img, outputs=output1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1bf9e-fc85-4f8a-86b4-e93b630e91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2, VGG19, VGG16\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "  \n",
    "\n",
    "def build_imagenet_unet(input_shape, n_filters=32, dropout=0.5, batchnorm=True):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"input_image\")\n",
    "    \n",
    "    encoder =  VGG19(input_tensor=inputs, weights=\"imagenet\", include_top=False)\n",
    "#     encoder.summary()    \n",
    "    \n",
    "    \n",
    "    c50 = encoder.get_layer(\"block5_conv4\").output\n",
    "    c51 = ASPP_block(c50)\n",
    "    c52 = Dropout(dropout)(c51)\n",
    "\n",
    "\n",
    "\n",
    "    # Decoder path\n",
    "    u6 = layers.UpSampling2D(size=(2, 2))(c52)\n",
    "    u62 = layers.Concatenate()([u6, res_block(encoder.get_layer(\"block4_conv4\").output, n_filters * 6, 1)])\n",
    "    u63 = conv2d_block(u62, n_filters * 6)\n",
    "    u64 = rrb_block2(u63, n_filters * 6, kernel_size=3, batchnorm=batchnorm)\n",
    "    d65 = Dropout(dropout)(u64)\n",
    "    \n",
    "    \n",
    "    u7 = layers.UpSampling2D(size=(2, 2))(d65)\n",
    "    u72 = layers.Concatenate()([u7, res_block(encoder.get_layer(\"block3_conv4\").output,n_filters * 4, 2)])\n",
    "    u73 = conv2d_block(u72, n_filters * 4)\n",
    "    u74 = rrb_block2(u73, n_filters * 4, kernel_size=3, batchnorm=batchnorm)\n",
    "    d75 = Dropout(dropout)(u74)\n",
    "\n",
    "    \n",
    "    \n",
    "    ### Decoder Full_Scale Up_Conv ####\n",
    "    f1 = layers.UpSampling2D(size=(4, 4))(d65)\n",
    "    ####################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    u8 = layers.UpSampling2D(size=(2, 2))(d75)\n",
    "    u82 = layers.Concatenate()([u8, f1, res_block(encoder.get_layer(\"block2_conv2\").output, n_filters * 2, 3)])\n",
    "    u83 = conv2d_block(u82, n_filters * 2)\n",
    "    u84 = rrb_block2(u83, n_filters * 2, kernel_size=3, batchnorm=batchnorm)\n",
    "    d85 = Dropout(dropout)(u84)\n",
    "\n",
    "\n",
    "    \n",
    "    ### Decoder Full_Scale Up_Conv ####\n",
    "    f2 = layers.UpSampling2D(size=(4, 4))(d75)\n",
    "    #####################################\n",
    "    u9 = layers.UpSampling2D(size=(2, 2))(d85)\n",
    "    u92 = layers.Concatenate()([u9, f2, res_block(encoder.get_layer(\"block1_conv2\").output, n_filters * 1, 4)])\n",
    "    u93 = conv2d_block(u92, n_filters * 1)\n",
    "    u94 = rrb_block2(u93, n_filters * 1, kernel_size=3, batchnorm=batchnorm)\n",
    "    d95 = Dropout(dropout)(u94)\n",
    "    \n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), padding='same', activation='sigmoid', name=\"out1\")(d95)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca377f-fff8-42df-9066-ddbfae5656d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (512, 512, 3)\n",
    "model = build_imagenet_unet(input_shape, n_filters=16, dropout=0.2, batchnorm=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(3e-4),\n",
    "              loss= weighted_bce_dice_loss,\n",
    "              metrics=[dice_acc,jacard_acc,\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2253b-5b32-4957-8cd2-fa001339fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_acc(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    " \n",
    "# jacard accuracy\n",
    "def jacard_acc(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "\n",
    "def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n",
    "    y_true_f = K.cast(y_true, 'float32')\n",
    "    y_pred_f = K.cast(y_pred, 'float32')\n",
    "    y_true_pos = K.flatten(y_true_f)\n",
    "    y_pred_pos = K.flatten(y_pred_f)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n",
    "    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n",
    "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n",
    "\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true, y_pred)\n",
    "\n",
    "\n",
    "def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n",
    "    tv = tversky(y_true, y_pred)\n",
    "    return K.pow((1 - tv), gamma)\n",
    "\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    # avoiding overflow\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\n",
    "    loss = (1. - y_true) * logit_y_pred + (1. + (weight - 1.) * y_true) * \\\n",
    "           (K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight * weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd number\n",
    "    averaged_mask = K.pool2d(\n",
    "        y_true, pool_size=(1, 1), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    border = K.cast(K.greater(averaged_mask, 0.15), 'float32') * K.cast(K.less(averaged_mask, 0.85), 'float32')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight += border * 2\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "#     loss = weighted_bce_loss(y_true, y_pred, weight) + focal_tversky_loss(y_true, y_pred) + weighted_dice_loss(y_true, y_pred, weight)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    def dice_loss(y_true, y_pred):\n",
    "        numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=(1,2,3))\n",
    "        denominator = tf.reduce_sum(y_true + y_pred, axis=(1,2,3))\n",
    "\n",
    "        return tf.reshape(1 - numerator / denominator, (-1, 1, 1))\n",
    "\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0e3e2-21d9-44da-9b66-e27e80d1525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_DSC(truth, pred, classes=2):\n",
    "    dice_scores = []\n",
    "    # Iterate over each class\n",
    "    for i in range(classes):\n",
    "        try:\n",
    "            gt = np.equal(truth, i)\n",
    "            pd = np.equal(pred, i)\n",
    "            # Calculate Dice\n",
    "            dice = 2*np.logical_and(pd, gt).sum() / (pd.sum() + gt.sum())\n",
    "            dice_scores.append(dice)\n",
    "        except ZeroDivisionError:\n",
    "            dice_scores.append(0.0)\n",
    "    # Return computed Dice Similarity Coefficients\n",
    "    return dice_scores[1]\n",
    "\n",
    "def calc_IoU(truth, pred, classes):\n",
    "    iou_scores = []\n",
    "    # Iterate over each class\n",
    "    for i in range(classes):\n",
    "        try:\n",
    "            gt = np.equal(truth, i)\n",
    "            pd = np.equal(pred, i)\n",
    "            # Calculate iou\n",
    "            iou = np.logical_and(pd, gt).sum() / (pd.sum() + gt.sum() - np.logical_and(pd, gt).sum())\n",
    "            iou_scores.append(iou)\n",
    "        except ZeroDivisionError:\n",
    "            iou_scores.append(0.0)\n",
    "    # Return computed IoU\n",
    "    return iou_scores[1]\n",
    "\n",
    "\n",
    "def calc_Sensitivity(truth, pred, classes):\n",
    "    sens_scores = []\n",
    "    # Iterate over each class\n",
    "    for i in range(classes):\n",
    "        try:\n",
    "            gt = np.equal(truth, i)\n",
    "            pd = np.equal(pred, i)\n",
    "            # Calculate sensitivity\n",
    "            sens = np.logical_and(pd, gt).sum() / gt.sum()\n",
    "            sens_scores.append(sens)\n",
    "        except ZeroDivisionError:\n",
    "            sens_scores.append(0.0)\n",
    "    # Return computed sensitivity scores\n",
    "    return sens_scores[1]\n",
    "\n",
    "def calc_Specificity(truth, pred, classes):\n",
    "    spec_scores = []\n",
    "    # Iterate over each class\n",
    "    for i in range(classes):\n",
    "        try:\n",
    "            not_gt = np.logical_not(np.equal(truth, i))\n",
    "            not_pd = np.logical_not(np.equal(pred, i))\n",
    "            # Calculate specificity\n",
    "            spec = np.logical_and(not_pd, not_gt).sum() / (not_gt).sum()\n",
    "            spec_scores.append(spec)\n",
    "        except ZeroDivisionError:\n",
    "            spec_scores.append(0.0)\n",
    "    # Return computed specificity scores\n",
    "    return spec_scores[1]\n",
    "\n",
    "def calc_Accuracy(truth, pred, classes):\n",
    "    acc_scores = []\n",
    "    # Iterate over each class\n",
    "    for i in range(classes):\n",
    "        try:\n",
    "            gt = np.equal(truth, i)\n",
    "            pd = np.equal(pred, i)\n",
    "            not_gt = np.logical_not(np.equal(truth, i))\n",
    "            not_pd = np.logical_not(np.equal(pred, i))\n",
    "            # Calculate accuracy\n",
    "            acc = (np.logical_and(pd, gt).sum() + \\\n",
    "                   np.logical_and(not_pd, not_gt).sum()) /  gt.size\n",
    "            acc_scores.append(acc)\n",
    "        except ZeroDivisionError:\n",
    "            acc_scores.append(0.0)\n",
    "    # Return computed accuracy scores\n",
    "    return acc_scores[1]\n",
    "\n",
    "def calc_Precision(truth, pred, classes):\n",
    "    prec_scores = []\n",
    "    # Iterate over each class\n",
    "    for i in range(classes):\n",
    "        try:\n",
    "            gt = np.equal(truth, i)\n",
    "            pd = np.equal(pred, i)\n",
    "            # Calculate precision\n",
    "            prec = np.logical_and(pd, gt).sum() / pd.sum()\n",
    "            prec_scores.append(prec)\n",
    "        except ZeroDivisionError:\n",
    "            prec_scores.append(0.0)\n",
    "    # Return computed precision scores\n",
    "    return prec_scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a72c1b-4afb-4b71-b873-f5e70926c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"./preprocessed_np\"\n",
    "\n",
    "x_train = np.load(os.path.join(OUT_DIR, \"x_train.npy\")) \n",
    "y_train = np.load(os.path.join(OUT_DIR, \"y_train.npy\"))   \n",
    "x_test  = np.load(os.path.join(OUT_DIR, \"x_test.npy\"))\n",
    "y_test  = np.load(os.path.join(OUT_DIR, \"y_test.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7140ea6-82c8-4873-9536-d0085a81205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "earlystop = EarlyStopping(monitor=\"val_dice_acc\", patience=20, verbose=1, mode=\"max\", restore_best_weights=True)\n",
    "ReduceLROnPlat = ReduceLROnPlateau(\n",
    "    monitor=\"val_dice_acc\", factor=0.1, patience=20, cooldown=3, min_lr=0, verbose=1, min_delta=0.0001\n",
    ")\n",
    "\n",
    "callbacks = [earlystop, ReduceLROnPlat, ClearMemory(), model_checkpoint]\n",
    "\n",
    "\n",
    "start = time()\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=global_batch_size,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n",
    "\n",
    "end = time()\n",
    "print(f\"\\nModel took {end - start:.2f} seconds to train\")\n",
    "print(f\"Model took {(end - start)/60:.2f} minutes to train\\n\")\n",
    "\n",
    "results = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7593f-b1bc-4710-9b61-95bff129f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_m = copy.deepcopy(results)\n",
    "act = copy.deepcopy(y_test)\n",
    "\n",
    "t = 0.5\n",
    "\n",
    "pred_m[pred_m >= t] = 1\n",
    "pred_m[pred_m < t] = 0\n",
    "\n",
    "act[act >= t] = 1\n",
    "act[act < t] = 0\n",
    "\n",
    "for x in range(pred_m.shape[0]):\n",
    "    tc = pred_m[x].flatten()\n",
    "    tc = collections.Counter(tc)\n",
    "    tv = tc[1]\n",
    "    if tv < 10000:\n",
    "        pred_m[x][pred_m[x] == 1] = 0\n",
    "\n",
    "dice_v = round(calc_DSC(act, pred_m, 2),4)\n",
    "iou_v = round(calc_IoU(act, pred_m, 2),4)\n",
    "sens_v = round(calc_Sensitivity(act, pred_m, 2),4)\n",
    "spec_v = round(calc_Specificity(act, pred_m, 2),4)\n",
    "prec_v = round(calc_Precision(act, pred_m, 2),4)\n",
    "auc_v = round(metrics.roc_auc_score(act.ravel(), pred_m.ravel()),4)\n",
    "acc_v = round(calc_Accuracy(act, pred_m, 2),4)\n",
    "\n",
    "print(\"Accuracy = \", acc_v)\n",
    "print(\"Precision = \", prec_v)\n",
    "print(\"F1/Dice = \", dice_v)\n",
    "print(\"IoU = \", iou_v)\n",
    "print(\"Sensitivity = \", sens_v)\n",
    "print(\"Specificity = \", spec_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa092d8-bf7c-4e19-ba54-4082760f18de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
